import json
import hashlib

def create_deduplicated_jsonl(clean_file_path):
    """
    Sadece geÃ§erli JSON satÄ±rlarÄ± iÃ§eren bir dosyayÄ± okur ve
    tekrar eden kayÄ±tlarÄ± temizleyerek yeni bir dosya oluÅŸturur.
    """
    # Ã‡Ä±ktÄ± dosyasÄ±nÄ±n ismini belirle
    base_name = clean_file_path.replace('.jsonl', '')
    deduplicated_file_path = f"{base_name}_tekrarsiz.jsonl"

    seen_hashes = set()
    written_count = 0
    duplicate_count = 0

    print(f"\n'{clean_file_path}' dosyasÄ±ndaki tekrarlar temizleniyor...")

    try:
        with open(clean_file_path, 'r', encoding='utf-8') as f_in, \
             open(deduplicated_file_path, 'w', encoding='utf-8') as f_out:

            for line in f_in:
                if not line.strip():
                    continue

                # Her ihtimale karÅŸÄ± satÄ±rÄ± tekrar parse ediyoruz
                data = json.loads(line)
                canonical_form = json.dumps(data, sort_keys=True)
                content_hash = hashlib.sha256(canonical_form.encode('utf-8')).hexdigest()

                if content_hash not in seen_hashes:
                    # Bu iÃ§eriÄŸi ilk defa gÃ¶rÃ¼yoruz
                    seen_hashes.add(content_hash)
                    f_out.write(line) # Orijinal satÄ±rÄ± yaz
                    written_count += 1
                else:
                    # Bu bir tekrar, atla
                    duplicate_count += 1
        
        print("\nÄ°ÅŸlem tamamlandÄ±!")
        print(f"âœ”ï¸ '{deduplicated_file_path}' dosyasÄ±na {written_count} adet benzersiz satÄ±r yazÄ±ldÄ±.")
        print(f"ğŸ—‘ï¸ Toplam {duplicate_count} adet tekrarlanan satÄ±r atlandÄ±.")


    except FileNotFoundError:
        print(f"Hata: '{clean_file_path}' adÄ±nda bir dosya bulunamadÄ±. LÃ¼tfen Ã¶nce 1. AdÄ±mÄ± Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan emin olun.")
    except Exception as e:
        print(f"Beklenmedik bir hata oluÅŸtu: {e}")


# --- KULLANIM ---
create_deduplicated_jsonl('/home/ika/llm/finetune/data/data_combined_temiz.jsonl')