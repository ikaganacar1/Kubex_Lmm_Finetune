# Default configuration template for fine-tuning
# Adjust values based on your hardware and dataset

model:
  name: "unsloth/Qwen2.5-3B-bnb-4bit"  # Smaller model for general use
  max_seq_length: 1024  # Conservative length
  dtype: null
  load_in_4bit: true
  trust_remote_code: true
  use_cache: false
  
  lora:
    r: 16  # Standard rank
    alpha: 16
    dropout: 0
    bias: "none"
    target_modules: [
      "q_proj", "k_proj", "v_proj", "o_proj",
      "gate_proj", "up_proj", "down_proj"
    ]
    gradient_checkpointing: "unsloth"
    random_state: 42
    use_rslora: false
    loftq_config: null

training:
  per_device_train_batch_size: 1  # Safe for most GPUs
  gradient_accumulation_steps: 8  # Effective batch size of 8
  num_train_epochs: 1  # Single epoch for testing
  learning_rate: 2e-4
  
  # Memory optimizations
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  optimizer: "adamw_8bit"
  
  # Precision settings (adjust based on GPU)
  bf16: false  # Set to true if GPU supports it
  fp16: true   # Fallback precision
  
  # Learning rate scheduling
  warmup_steps: 10
  warmup_ratio: 0.1
  lr_scheduler_type: "linear"
  
  # Logging and saving
  logging_steps: 10
  logging_first_step: true
  report_to: "none"
  
  save_strategy: "epoch"
  save_steps: 500
  save_total_limit: 1
  eval_strategy: "no"
  
  # Dataset optimizations
  packing: false
  dataloader_pin_memory: false
  remove_unused_columns: true
  dataset_num_proc: 1
  seed: 42

dataset:
  path: "your_dataset.jsonl"  # Replace with your dataset path
  format: "jsonl"  # json, jsonl, or csv
  conversation_format: "chatml"  # chatml, alpaca, or simple_qa
  max_length: 2048  # Filter examples longer than this
  subset_size: 0  # 0 = use full dataset, >0 = use subset

hardware:
  device_map: "auto"
  environment_variables:
    PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128"
    CUDA_LAUNCH_BLOCKING: "1"
    TORCH_USE_CUDA_DSA: "1"
    TOKENIZERS_PARALLELISM: "false"

output:
  directory: "./finetuned-model"
  save_method: "merged_16bit"  # merged_16bit, merged_4bit, or lora
  
  test_prompts:
    - "Hello, how can you help me?"
    - "Explain a complex concept simply"
    - "Write a short story about AI"

# Hardware-specific configurations (uncomment as needed)

# For RTX 4090/A100 (high VRAM):
# training:
#   per_device_train_batch_size: 8
#   gradient_accumulation_steps: 2
# model:
#   max_seq_length: 4096

# For RTX 3060/4060 (low VRAM):
# training:
#   per_device_train_batch_size: 1
#   gradient_accumulation_steps: 16
# model:
#   max_seq_length: 512
#   name: "unsloth/tinyllama-bnb-4bit"

# For multi-GPU setups:
# hardware:
#   environment_variables:
#     CUDA_VISIBLE_DEVICES: "0,1"
# training:
#   per_device_train_batch_size: 2  # Per GPU