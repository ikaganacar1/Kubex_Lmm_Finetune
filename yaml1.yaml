# Optimized Smart Training Config for RTX 3090 24GB + 25k Kubernetes Dataset
# With advanced early stopping and auto-optimization

model:
  name: "unsloth/Qwen2.5-7B-bnb-4bit"
  max_seq_length: 2048
  dtype: null
  load_in_4bit: true
  trust_remote_code: true
  use_cache: false
  
  lora:
    r: 32
    alpha: 32
    dropout: 0.05
    bias: "none"
    target_modules: [
      "q_proj", "k_proj", "v_proj", "o_proj",
      "gate_proj", "up_proj", "down_proj"
    ]
    gradient_checkpointing: "unsloth"
    random_state: 42
    use_rslora: true
    loftq_config: null

training:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  num_train_epochs: 3
  learning_rate: 0.0001
  max_steps: -1
  
  gradient_checkpointing: true
  optimizer: "adamw_8bit"
  bf16: true
  fp16: false
  
  warmup_steps: 100
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  
  logging_steps: 25
  save_strategy: "steps"
  save_steps: 250
  save_total_limit: 4
  report_to: "none"
  
  packing: false
  seed: 42

dataset:
  path: "kubernetes_chatml.jsonl"
  format: "jsonl"
  conversation_format: "chatml"
  max_length: 4096
  subset_size: 0

hardware:
  device_map: "auto"
  environment_variables:
    PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:256"
    TOKENIZERS_PARALLELISM: "false"
    CUDA_VISIBLE_DEVICES: "0"

smart_training:
  enable_loss_early_stopping: true
  early_stop_patience: 10
  early_stop_min_delta: 0.005
  early_stop_min_steps: 300
  early_stop_check_interval: 25
  
  target_loss: 1.2
  max_time_minutes: 30000
  max_steps: null
  
  dataset_num_proc: 4

output:
  directory: "./qwen-kubernetes-expert"
  save_method: "merged_16bit"
  
  test_prompts:
    - "How do I create a Kubernetes deployment with 3 replicas and resource limits?"
    - "What's the difference between a Service and an Ingress in Kubernetes?"
    - "How can I debug a pod that's stuck in Pending state?"
    - "Explain Kubernetes ConfigMaps and Secrets with examples"
    - "How do I set up horizontal pod autoscaling based on CPU usage?"