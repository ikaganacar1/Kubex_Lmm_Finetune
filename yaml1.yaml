# Recommended configuration for RTX 3090 24GB with 25k Kubernetes Q&A dataset
# This config is optimized for your specific hardware and dataset size

model:
  name: "unsloth/Qwen2.5-7B-bnb-4bit"  # Good balance of capability and memory usage
  max_seq_length: 2048  # Good for Kubernetes documentation/code
  dtype: null  # Auto-detect
  load_in_4bit: true  # Essential for 24GB GPU
  trust_remote_code: true
  use_cache: false
  
  lora:
    r: 32  # Higher rank for better quality with your large dataset
    alpha: 32  # Match with rank
    dropout: 0.05  # Slight dropout for regularization with large dataset
    bias: "none"
    target_modules: [
      "q_proj", "k_proj", "v_proj", "o_proj",
      "gate_proj", "up_proj", "down_proj"
    ]
    gradient_checkpointing: "unsloth"
    random_state: 42
    use_rslora: true  # Better stability for large datasets
    loftq_config: null

training:
  per_device_train_batch_size: 4  # Optimized for 24GB
  gradient_accumulation_steps: 4  # Effective batch size of 16
  num_train_epochs: 3  # Multiple epochs for 25k samples
  learning_rate: 0.0001  # 1e-4 - Slightly lower for stability
  
  # Memory optimizations
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  optimizer: "adamw_8bit"
  
  # Precision settings (RTX 3090 supports bf16)
  bf16: true
  fp16: false
  
  # Learning rate scheduling
  warmup_steps: 100  # More warmup for large dataset
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"  # Better for multi-epoch training
  
  # Logging and saving
  logging_steps: 25
  logging_first_step: true
  report_to: "none"  # Change to "wandb" if you want tracking
  
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 3
  # Early stopping (optional)
  eval_strategy: "steps"
  eval_steps: 200
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  early_stopping_patience: 3  # Stop if no improvement for 3 evaluations
  
  # Dataset optimizations
  packing: false  # Better quality for Q&A format
  dataloader_pin_memory: false
  remove_unused_columns: true
  dataset_num_proc: 4  # Use multiple cores for preprocessing
  seed: 42

dataset:
  path: "kubernetes_chatml.jsonl"  # Your dataset file
  format: "jsonl"  # or "json" depending on your file
  conversation_format: "chatml"  # Qwen format
  max_length: 4096  # Filter out extremely long examples
  subset_size: 0  # Use full dataset (25k samples)

hardware:
  device_map: "auto"
  environment_variables:
    PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:256"  # Larger chunks for 24GB
    CUDA_LAUNCH_BLOCKING: "0"  # Disable for better performance
    TORCH_USE_CUDA_DSA: "1"
    TOKENIZERS_PARALLELISM: "false"
    CUDA_VISIBLE_DEVICES: "0"  # Use first GPU

output:
  directory: "./qwen-kubernetes-expert"
  save_method: "merged_16bit"  # Best compatibility
  
  # Test prompts to verify model after training
  test_prompts:
    - "How do I create a Kubernetes deployment with 3 replicas?"
    - "What's the difference between a Service and an Ingress in Kubernetes?"
    - "How can I debug a pod that's stuck in Pending state?"
    - "Explain Kubernetes resource limits and requests"
    - "How do I set up horizontal pod autoscaling?"