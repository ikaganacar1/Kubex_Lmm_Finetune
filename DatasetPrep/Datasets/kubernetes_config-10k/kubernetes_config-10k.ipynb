{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d5ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'train.parquet', 'test': 'test.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/HelloBoieeee/kubernetes_config/\" + splits[\"train\"])\n",
    "df.to_json(\"kubernetes_config-10k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ba416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Can you generate a K8S deployment config file?[/INST]apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: open-api-doc\n",
      "spec:\n",
      "  type: ClusterIP\n",
      "  ports:\n",
      "  - port: 80\n",
      "    targetPort: 80\n",
      "    protocol: TCP\n",
      "    name: http\n",
      "  selector:\n",
      "    app: open-api-doc</s>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('kubernetes_config-10k.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data.get(\"text\").get(\"0\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f120a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "class KubernetesDatasetCleaner:\n",
    "    def __init__(self):\n",
    "        # Compile regex patterns for better performance\n",
    "        self.patterns = {\n",
    "            # Special tokens for this dataset\n",
    "            'inst_tags': re.compile(r'<s>\\[INST\\](.*?)\\[/INST\\](.*?)</s>', re.DOTALL),\n",
    "            'start_token': re.compile(r'^<s>'),\n",
    "            'end_token': re.compile(r'</s>$'),\n",
    "            'inst_start': re.compile(r'\\[INST\\]'),\n",
    "            'inst_end': re.compile(r'\\[/INST\\]'),\n",
    "            \n",
    "            # General cleaning patterns\n",
    "            'html_tags': re.compile(r'<[^>]+>'),\n",
    "            'multiple_newlines': re.compile(r'\\n\\s*\\n\\s*\\n+'),\n",
    "            'multiple_spaces': re.compile(r'[ \\t]+'),\n",
    "            'leading_trailing_space': re.compile(r'^\\s+|\\s+$', re.MULTILINE)\n",
    "        }\n",
    "    \n",
    "    def extract_conversation(self, text):\n",
    "        \"\"\"\n",
    "        Extract user question and assistant response from the special token format\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return None, None\n",
    "        \n",
    "        # Try to match the full pattern: <s>[INST]question[/INST]answer</s>\n",
    "        match = self.patterns['inst_tags'].search(text)\n",
    "        if match:\n",
    "            user_content = match.group(1).strip()\n",
    "            assistant_content = match.group(2).strip()\n",
    "            return user_content, assistant_content\n",
    "        \n",
    "        # Fallback: try to split on [/INST] if full pattern doesn't work\n",
    "        if '[INST]' in text and '[/INST]' in text:\n",
    "            # Remove start/end tokens\n",
    "            text = self.patterns['start_token'].sub('', text)\n",
    "            text = self.patterns['end_token'].sub('', text)\n",
    "            \n",
    "            # Split on [/INST]\n",
    "            parts = text.split('[/INST]')\n",
    "            if len(parts) == 2:\n",
    "                user_content = parts[0].replace('[INST]', '').strip()\n",
    "                assistant_content = parts[1].strip()\n",
    "                return user_content, assistant_content\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean text by removing extra whitespace and formatting\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string if not already\n",
    "        text = str(text)\n",
    "        \n",
    "        # Remove any remaining HTML tags (just in case)\n",
    "        text = self.patterns['html_tags'].sub('', text)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        text = self.patterns['multiple_newlines'].sub('\\n\\n', text)\n",
    "        text = self.patterns['multiple_spaces'].sub(' ', text)\n",
    "        text = self.patterns['leading_trailing_space'].sub('', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def is_valid_content(self, text):\n",
    "        \"\"\"\n",
    "        Check if the content is valid (not empty, has meaningful content)\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) < 5:\n",
    "            return False\n",
    "        \n",
    "        # Check if it has some alphanumeric content\n",
    "        alphanumeric = re.sub(r'[^a-zA-Z0-9]', '', text)\n",
    "        if len(alphanumeric) < 3:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def clean_and_convert_dataset(self, input_file, output_file):\n",
    "        \"\"\"\n",
    "        Clean the Kubernetes dataset and convert to ChatML format\n",
    "        \"\"\"\n",
    "        print(f\"Loading Kubernetes dataset from {input_file}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(input_file, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            return 0\n",
    "        \n",
    "        text_data = data.get(\"text\", {})\n",
    "        print(f\"Found {len(text_data)} text entries\")\n",
    "        \n",
    "        cleaned_conversations = []\n",
    "        skipped = 0\n",
    "        \n",
    "        # Get all keys and sort them numerically\n",
    "        keys = list(text_data.keys())\n",
    "        try:\n",
    "            # Try to sort numerically if keys are string numbers\n",
    "            keys = sorted(keys, key=lambda x: int(x))\n",
    "        except ValueError:\n",
    "            # If not all numeric, sort alphabetically\n",
    "            keys = sorted(keys)\n",
    "        \n",
    "        print(f\"Processing {len(keys)} entries...\")\n",
    "        \n",
    "        for i, key in enumerate(keys):\n",
    "            try:\n",
    "                raw_text = text_data[key]\n",
    "                \n",
    "                # Extract user question and assistant response\n",
    "                user_content, assistant_content = self.extract_conversation(raw_text)\n",
    "                \n",
    "                if user_content is None or assistant_content is None:\n",
    "                    skipped += 1\n",
    "                    if i % 1000 == 0 and i > 0:\n",
    "                        print(f\"Processed {i}/{len(keys)} entries, skipped {skipped} so far...\")\n",
    "                    continue\n",
    "                \n",
    "                # Clean the content\n",
    "                clean_user = self.clean_text(user_content)\n",
    "                clean_assistant = self.clean_text(assistant_content)\n",
    "                \n",
    "                # Validate content\n",
    "                if not self.is_valid_content(clean_user) or not self.is_valid_content(clean_assistant):\n",
    "                    skipped += 1\n",
    "                    if i % 1000 == 0 and i > 0:\n",
    "                        print(f\"Processed {i}/{len(keys)} entries, skipped {skipped} so far...\")\n",
    "                    continue\n",
    "                \n",
    "                # Create ChatML conversation\n",
    "                conversation = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": clean_user\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": clean_assistant\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                cleaned_conversations.append(conversation)\n",
    "                \n",
    "                # Progress update\n",
    "                if i % 1000 == 0 and i > 0:\n",
    "                    print(f\"Processed {i}/{len(keys)} entries, {len(cleaned_conversations)} valid conversations...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing entry {key}: {e}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "        # Save to JSONL format\n",
    "        print(f\"Saving {len(cleaned_conversations)} conversations to {output_file}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                for conversation in cleaned_conversations:\n",
    "                    file.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving dataset: {e}\")\n",
    "            return 0\n",
    "        \n",
    "        print(f\"‚úÖ Successfully cleaned and converted {len(cleaned_conversations)} conversations\")\n",
    "        print(f\"‚ùå Skipped {skipped} invalid entries\")\n",
    "        print(f\"üìÅ Output saved to: {output_file}\")\n",
    "        \n",
    "        return len(cleaned_conversations)\n",
    "    \n",
    "    def preview_cleaned_data(self, output_file, num_examples=3):\n",
    "        \"\"\"\n",
    "        Preview the cleaned data\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PREVIEW OF CLEANED KUBERNETES DATA\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, 'r', encoding='utf-8') as file:\n",
    "                for i, line in enumerate(file):\n",
    "                    if i >= num_examples:\n",
    "                        break\n",
    "                    \n",
    "                    conversation = json.loads(line)\n",
    "                    print(f\"\\nüìù CONVERSATION {i+1}:\")\n",
    "                    print(f\"{'‚îÄ'*40}\")\n",
    "                    \n",
    "                    user_msg = conversation[\"messages\"][0][\"content\"]\n",
    "                    assistant_msg = conversation[\"messages\"][1][\"content\"]\n",
    "                    \n",
    "                    print(f\"üë§ USER ({len(user_msg)} chars):\")\n",
    "                    print(user_msg)\n",
    "                    \n",
    "                    print(f\"\\nü§ñ ASSISTANT ({len(assistant_msg)} chars):\")\n",
    "                    print(assistant_msg[:500] + \"...\" if len(assistant_msg) > 500 else assistant_msg)\n",
    "                    \n",
    "                    print(f\"{'‚îÄ'*40}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error previewing data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91593688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Kubernetes Dataset Cleaner and ChatML Converter\n",
      "==================================================\n",
      "Loading Kubernetes dataset from kubernetes_config-10k.json...\n",
      "Found 10000 text entries\n",
      "Processing 10000 entries...\n",
      "Processed 1000/10000 entries, 1001 valid conversations...\n",
      "Processed 2000/10000 entries, 2001 valid conversations...\n",
      "Processed 3000/10000 entries, 3001 valid conversations...\n",
      "Processed 4000/10000 entries, 4001 valid conversations...\n",
      "Processed 5000/10000 entries, 5001 valid conversations...\n",
      "Processed 6000/10000 entries, 6001 valid conversations...\n",
      "Processed 7000/10000 entries, 7001 valid conversations...\n",
      "Processed 8000/10000 entries, 8001 valid conversations...\n",
      "Processed 9000/10000 entries, 9001 valid conversations...\n",
      "Saving 10000 conversations to kubernetes_config-10k.jsonl...\n",
      "‚úÖ Successfully cleaned and converted 10000 conversations\n",
      "‚ùå Skipped 0 invalid entries\n",
      "üìÅ Output saved to: kubernetes_config-10k.jsonl\n",
      "\n",
      "============================================================\n",
      "PREVIEW OF CLEANED KUBERNETES DATA\n",
      "============================================================\n",
      "\n",
      "üìù CONVERSATION 1:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üë§ USER (46 chars):\n",
      "Can you generate a K8S deployment config file?\n",
      "\n",
      "ü§ñ ASSISTANT (165 chars):\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "name: open-api-doc\n",
      "spec:\n",
      "type: ClusterIP\n",
      "ports:\n",
      "- port: 80\n",
      "targetPort: 80\n",
      "protocol: TCP\n",
      "name: http\n",
      "selector:\n",
      "app: open-api-doc\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìù CONVERSATION 2:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üë§ USER (46 chars):\n",
      "Can you generate a K8S deployment config file?\n",
      "\n",
      "ü§ñ ASSISTANT (156 chars):\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "name: argocd-metrics\n",
      "spec:\n",
      "ports:\n",
      "- name: http\n",
      "protocol: TCP\n",
      "port: 8082\n",
      "targetPort: 8082\n",
      "selector:\n",
      "app: argocd-server\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚ú® Processing complete!\n",
      "üìä Statistics:\n",
      "   - Input file: kubernetes_config-10k.json\n",
      "   - Output file: kubernetes_config-10k.jsonl\n",
      "   - Valid conversations: 10000\n",
      "   - Ready for training! üöÄ\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the Kubernetes dataset cleaning and conversion\n",
    "    \"\"\"\n",
    "    print(\"üßπ Kubernetes Dataset Cleaner and ChatML Converter\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    cleaner = KubernetesDatasetCleaner()\n",
    "    \n",
    "    # Clean and convert the dataset\n",
    "    input_file = 'kubernetes_config-10k.json'\n",
    "    output_file = 'kubernetes_config-10k.jsonl'\n",
    "    \n",
    "    num_conversations = cleaner.clean_and_convert_dataset(input_file, output_file)\n",
    "    \n",
    "    # Preview the results if successful\n",
    "    if num_conversations > 0:\n",
    "        cleaner.preview_cleaned_data(output_file, num_examples=2)\n",
    "        \n",
    "        print(f\"\\n‚ú® Processing complete!\")\n",
    "        print(f\"üìä Statistics:\")\n",
    "        print(f\"   - Input file: {input_file}\")\n",
    "        print(f\"   - Output file: {output_file}\")\n",
    "        print(f\"   - Valid conversations: {num_conversations}\")\n",
    "        print(f\"   - Ready for training! üöÄ\")\n",
    "    else:\n",
    "        print(\"‚ùå No valid conversations were generated. Check your input file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
