{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf56188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/mcipriano/stackoverflow-kubernetes-questions/data/kubernetes_dump.parquet\")\n",
    "df.to_json(\"kubernetes-sof-qa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eacccf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>How to resolve the error no module named pandas when one node (in Airflow's DAG) is successful in using it(pandas) and the other is not?</p>\n",
      "\n",
      "<p>I am unable to deduce as to why I am getting an error no module named pandas.</p>\n",
      "\n",
      "<p>I have checked via <code>pip3 freeze</code> and yes, the desired pandas version does show up.</p>\n",
      "\n",
      "<p>I have deployed this using docker on a kubernetes cluster.</p>\n",
      "\n",
      "\n",
      "<p><a href=\"https://github.com/apache/incubator-airflow/blob/v1-10-stable/setup.py#L292\" rel=\"nofollow noreferrer\">Pandas is generally required</a>, and sometimes used in some hooks to return dataframes. Well, it's possible that Airflow was installed with <code>pip</code> and not <code>pip3</code> possibly being added as a Python 2 module and not a Python 3 module (though, using <code>pip</code> should have installed Pandas when one looks at the <a href=\"https://github.com/apache/incubator-airflow/blob/v1-10-stable/setup.py#L292\" rel=\"nofollow noreferrer\"><code>setup.py</code></a>).</p>\n",
      "\n",
      "<p>Which Operator in your DAG is giving this error?\n",
      "Do you have any PythonVirtualEnvironmentOperators or BashOperators running <code>python</code> from the command line (and thus possibly not sharing the same environment that you're checking has <code>pandas</code>)?</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('kubernetes-sof-qa-30k.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data.get(\"Question\")[\"0\"])\n",
    "print()\n",
    "print(data.get(\"Answer\")[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54301987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "class KubernetesSOFDatasetCleaner:\n",
    "    def __init__(self):\n",
    "        # Compile regex patterns for better performance\n",
    "        self.patterns = {\n",
    "            # HTML cleaning\n",
    "            'html_tags': re.compile(r'<[^>]+>'),\n",
    "            'code_blocks': re.compile(r'<code>(.*?)</code>', re.DOTALL),\n",
    "            'paragraph_tags': re.compile(r'</?p>'),\n",
    "            'link_tags': re.compile(r'<a[^>]*>(.*?)</a>', re.DOTALL),\n",
    "            \n",
    "            # Whitespace cleaning\n",
    "            'multiple_newlines': re.compile(r'\\n\\s*\\n\\s*\\n+'),\n",
    "            'multiple_spaces': re.compile(r'[ \\t]+'),\n",
    "            'leading_trailing_space': re.compile(r'^\\s+|\\s+$', re.MULTILINE),\n",
    "            \n",
    "            # Content validation\n",
    "            'meaningful_words': re.compile(r'\\b[a-zA-Z]{3,}\\b')\n",
    "        }\n",
    "    \n",
    "    def clean_html_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean HTML content while preserving code blocks and structure\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string if not already\n",
    "        text = str(text)\n",
    "        \n",
    "        # Unescape HTML entities first\n",
    "        text = unescape(text)\n",
    "        \n",
    "        # Preserve code blocks by replacing them with placeholders\n",
    "        code_blocks = []\n",
    "        def preserve_code(match):\n",
    "            code_blocks.append(match.group(1))\n",
    "            return f\"__CODE_BLOCK_{len(code_blocks)-1}__\"\n",
    "        \n",
    "        text = self.patterns['code_blocks'].sub(preserve_code, text)\n",
    "        \n",
    "        # Extract link text (keep the text, remove the link)\n",
    "        text = self.patterns['link_tags'].sub(r'\\1', text)\n",
    "        \n",
    "        # Remove paragraph tags but keep the content\n",
    "        text = self.patterns['paragraph_tags'].sub('', text)\n",
    "        \n",
    "        # Remove remaining HTML tags\n",
    "        text = self.patterns['html_tags'].sub('', text)\n",
    "        \n",
    "        # Restore code blocks with backticks\n",
    "        for i, code_content in enumerate(code_blocks):\n",
    "            # Clean the code content\n",
    "            code_content = code_content.strip()\n",
    "            # Use backticks for inline code or code blocks\n",
    "            if '\\n' in code_content:\n",
    "                code_replacement = f\"```\\n{code_content}\\n```\"\n",
    "            else:\n",
    "                code_replacement = f\"`{code_content}`\"\n",
    "            text = text.replace(f\"__CODE_BLOCK_{i}__\", code_replacement)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        text = self.patterns['multiple_newlines'].sub('\\n\\n', text)\n",
    "        text = self.patterns['multiple_spaces'].sub(' ', text)\n",
    "        text = self.patterns['leading_trailing_space'].sub('', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def is_valid_content(self, text):\n",
    "        \"\"\"\n",
    "        Check if the content is valid (not empty, has meaningful content)\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return False\n",
    "        \n",
    "        # Check if it has some meaningful words\n",
    "        words = self.patterns['meaningful_words'].findall(text)\n",
    "        if len(words) < 3:\n",
    "            return False\n",
    "        \n",
    "        # Check if it's not just whitespace and punctuation\n",
    "        alphanumeric = re.sub(r'[^a-zA-Z0-9]', '', text)\n",
    "        if len(alphanumeric) < 5:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def clean_and_convert_dataset(self, input_file, output_file):\n",
    "        \"\"\"\n",
    "        Clean the Kubernetes Stack Overflow dataset and convert to ChatML format\n",
    "        \"\"\"\n",
    "        print(f\"Loading Kubernetes Stack Overflow dataset from {input_file}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(input_file, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            return 0\n",
    "        \n",
    "        questions = data.get(\"Question\", {})\n",
    "        answers = data.get(\"Answer\", {})\n",
    "        \n",
    "        print(f\"Found {len(questions)} questions and {len(answers)} answers\")\n",
    "        \n",
    "        cleaned_conversations = []\n",
    "        skipped = 0\n",
    "        \n",
    "        # Get all keys and sort them numerically\n",
    "        question_keys = set(questions.keys())\n",
    "        answer_keys = set(answers.keys())\n",
    "        all_keys = question_keys.intersection(answer_keys)  # Only process pairs that exist\n",
    "        \n",
    "        try:\n",
    "            # Try to sort numerically if keys are string numbers\n",
    "            sorted_keys = sorted(all_keys, key=lambda x: int(x))\n",
    "        except ValueError:\n",
    "            # If not all numeric, sort alphabetically\n",
    "            sorted_keys = sorted(all_keys)\n",
    "        \n",
    "        print(f\"Processing {len(sorted_keys)} question-answer pairs...\")\n",
    "        \n",
    "        for i, key in enumerate(sorted_keys):\n",
    "            try:\n",
    "                # Get raw content\n",
    "                raw_question = questions.get(key, \"\")\n",
    "                raw_answer = answers.get(key, \"\")\n",
    "                \n",
    "                # Clean the HTML content\n",
    "                clean_question = self.clean_html_text(raw_question)\n",
    "                clean_answer = self.clean_html_text(raw_answer)\n",
    "                \n",
    "                # Validate content\n",
    "                if not self.is_valid_content(clean_question) or not self.is_valid_content(clean_answer):\n",
    "                    skipped += 1\n",
    "                    if i % 1000 == 0 and i > 0:\n",
    "                        print(f\"Processed {i}/{len(sorted_keys)} entries, skipped {skipped} so far...\")\n",
    "                    continue\n",
    "                \n",
    "                # Create ChatML conversation\n",
    "                conversation = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": clean_question\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": clean_answer\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                cleaned_conversations.append(conversation)\n",
    "                \n",
    "                # Progress update\n",
    "                if i % 1000 == 0 and i > 0:\n",
    "                    print(f\"Processed {i}/{len(sorted_keys)} entries, {len(cleaned_conversations)} valid conversations...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing entry {key}: {e}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "        # Save to JSONL format\n",
    "        print(f\"Saving {len(cleaned_conversations)} conversations to {output_file}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                for conversation in cleaned_conversations:\n",
    "                    file.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving dataset: {e}\")\n",
    "            return 0\n",
    "        \n",
    "        print(f\"‚úÖ Successfully cleaned and converted {len(cleaned_conversations)} conversations\")\n",
    "        print(f\"‚ùå Skipped {skipped} invalid entries\")\n",
    "        print(f\"üìÅ Output saved to: {output_file}\")\n",
    "        \n",
    "        return len(cleaned_conversations)\n",
    "    \n",
    "    def preview_cleaned_data(self, output_file, num_examples=2):\n",
    "        \"\"\"\n",
    "        Preview the cleaned data\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PREVIEW OF CLEANED KUBERNETES STACK OVERFLOW DATA\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, 'r', encoding='utf-8') as file:\n",
    "                for i, line in enumerate(file):\n",
    "                    if i >= num_examples:\n",
    "                        break\n",
    "                    \n",
    "                    conversation = json.loads(line)\n",
    "                    print(f\"\\nüìù CONVERSATION {i+1}:\")\n",
    "                    print(f\"{'‚îÄ'*50}\")\n",
    "                    \n",
    "                    user_msg = conversation[\"messages\"][0][\"content\"]\n",
    "                    assistant_msg = conversation[\"messages\"][1][\"content\"]\n",
    "                    \n",
    "                    print(f\"üë§ USER QUESTION ({len(user_msg)} chars):\")\n",
    "                    print(user_msg)\n",
    "                    \n",
    "                    print(f\"\\nü§ñ ASSISTANT ANSWER ({len(assistant_msg)} chars):\")\n",
    "                    print(assistant_msg[:400] + \"...\" if len(assistant_msg) > 400 else assistant_msg)\n",
    "                    \n",
    "                    print(f\"{'‚îÄ'*50}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error previewing data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "450f85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Kubernetes Stack Overflow Dataset Cleaner and ChatML Converter\n",
      "============================================================\n",
      "Loading Kubernetes Stack Overflow dataset from kubernetes-sof-qa-30k.json...\n",
      "Found 30044 questions and 30044 answers\n",
      "Processing 30044 question-answer pairs...\n",
      "Processed 1000/30044 entries, 1001 valid conversations...\n",
      "Processed 2000/30044 entries, 2001 valid conversations...\n",
      "Processed 3000/30044 entries, 3001 valid conversations...\n",
      "Processed 4000/30044 entries, 4001 valid conversations...\n",
      "Processed 5000/30044 entries, 5001 valid conversations...\n",
      "Processed 6000/30044 entries, 6001 valid conversations...\n",
      "Processed 7000/30044 entries, 7001 valid conversations...\n",
      "Processed 8000/30044 entries, 8001 valid conversations...\n",
      "Processed 9000/30044 entries, 9001 valid conversations...\n",
      "Processed 10000/30044 entries, 10001 valid conversations...\n",
      "Processed 11000/30044 entries, 11001 valid conversations...\n",
      "Processed 12000/30044 entries, 12001 valid conversations...\n",
      "Processed 13000/30044 entries, 13001 valid conversations...\n",
      "Processed 14000/30044 entries, 14001 valid conversations...\n",
      "Processed 15000/30044 entries, 15001 valid conversations...\n",
      "Processed 16000/30044 entries, 16001 valid conversations...\n",
      "Processed 17000/30044 entries, 17001 valid conversations...\n",
      "Processed 18000/30044 entries, 18001 valid conversations...\n",
      "Processed 19000/30044 entries, 19001 valid conversations...\n",
      "Processed 20000/30044 entries, 20001 valid conversations...\n",
      "Processed 21000/30044 entries, 21001 valid conversations...\n",
      "Processed 22000/30044 entries, 22001 valid conversations...\n",
      "Processed 23000/30044 entries, 23001 valid conversations...\n",
      "Processed 24000/30044 entries, 24001 valid conversations...\n",
      "Processed 25000/30044 entries, 25001 valid conversations...\n",
      "Processed 26000/30044 entries, 26001 valid conversations...\n",
      "Processed 27000/30044 entries, 27001 valid conversations...\n",
      "Processed 28000/30044 entries, 28001 valid conversations...\n",
      "Processed 29000/30044 entries, 29001 valid conversations...\n",
      "Processed 30000/30044 entries, 30000 valid conversations...\n",
      "Saving 30043 conversations to kubernetes-sof-qa-30k-clean.jsonl...\n",
      "‚úÖ Successfully cleaned and converted 30043 conversations\n",
      "‚ùå Skipped 1 invalid entries\n",
      "üìÅ Output saved to: kubernetes-sof-qa-30k-clean.jsonl\n",
      "\n",
      "======================================================================\n",
      "PREVIEW OF CLEANED KUBERNETES STACK OVERFLOW DATA\n",
      "======================================================================\n",
      "\n",
      "üìù CONVERSATION 1:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üë§ USER QUESTION (353 chars):\n",
      "How to resolve the error no module named pandas when one node (in Airflow's DAG) is successful in using it(pandas) and the other is not?I am unable to deduce as to why I am getting an error no module named pandas.I have checked via `pip3 freeze` and yes, the desired pandas version does show up.I have deployed this using docker on a kubernetes cluster.\n",
      "\n",
      "ü§ñ ASSISTANT ANSWER (552 chars):\n",
      "Pandas is generally required, and sometimes used in some hooks to return dataframes. Well, it's possible that Airflow was installed with `pip` and not `pip3` possibly being added as a Python 2 module and not a Python 3 module (though, using `pip` should have installed Pandas when one looks at the `setup.py`).Which Operator in your DAG is giving this error?\n",
      "Do you have any PythonVirtualEnvironmentO...\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìù CONVERSATION 2:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üë§ USER QUESTION (863 chars):\n",
      "I tried to install ibm-eventstreams-dev v 0.1.2 into my Mac.After I installed eventstreams into my Mac, there's always several pods that can't run. It includes three kafka pods: es-ibm-es-kafka-sts-0/1/2, es-ibm-es-ui-deploy-69758d9dfd-kc2zx, es-ibm-es-ui-oauth2-client-reg-pgvq6 and there also have a failed job named es-ibm-es-ui-oauth2-client-reg.You can see the details in the follow images:So I have two questions about the ibm-event-stream:Does ibm-eventstreams-dev only supported on ICP? Can I install it on my local environment without ICP environment?\n",
      "How could I solve the ui pods problem in the ibm-eventstreams-dev?\n",
      "what's wrong with the kafka pods? what's the status message \"CrashLoopBackOff\" means?My environment details:kubernetes 1.11.1\n",
      "helm : stable 2.10.0\n",
      "a cluster have three nodes, each nodes is a virtual macine.Please help me, Thanks a lot!\n",
      "\n",
      "ü§ñ ASSISTANT ANSWER (1058 chars):\n",
      "So I have two questions about the ibm-event-stream:\n",
      "Does ibm-eventstreams-dev only supported on ICP? Can I install it on my local environment without ICP environment?Event Streams will only run on IBM Cloud Private (ICP). That's because ICP provides more than just a Kubernetes environment. For example, authentication and user management for Event Streams is provided by the ICP platform.That's what...\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚ú® Processing complete!\n",
      "üìä Statistics:\n",
      "   - Input file: kubernetes-sof-qa-30k.json\n",
      "   - Output file: kubernetes-sof-qa-30k-clean.jsonl\n",
      "   - Valid conversations: 30043\n",
      "   - Ready for training! üöÄ\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the Kubernetes Stack Overflow dataset cleaning and conversion\n",
    "    \"\"\"\n",
    "    print(\"üßπ Kubernetes Stack Overflow Dataset Cleaner and ChatML Converter\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cleaner = KubernetesSOFDatasetCleaner()\n",
    "    \n",
    "    # Clean and convert the dataset\n",
    "    input_file = 'kubernetes-sof-qa-30k.json'\n",
    "    output_file = 'kubernetes-sof-qa-30k-clean.jsonl'\n",
    "    \n",
    "    num_conversations = cleaner.clean_and_convert_dataset(input_file, output_file)\n",
    "    \n",
    "    # Preview the results if successful\n",
    "    if num_conversations > 0:\n",
    "        cleaner.preview_cleaned_data(output_file, num_examples=2)\n",
    "        \n",
    "        print(f\"\\n‚ú® Processing complete!\")\n",
    "        print(f\"üìä Statistics:\")\n",
    "        print(f\"   - Input file: {input_file}\")\n",
    "        print(f\"   - Output file: {output_file}\")\n",
    "        print(f\"   - Valid conversations: {num_conversations}\")\n",
    "        print(f\"   - Ready for training! üöÄ\")\n",
    "    else:\n",
    "        print(\"‚ùå No valid conversations were generated. Check your input file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
